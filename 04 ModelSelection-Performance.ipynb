{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd023eb-e6ca-499e-bda7-981768f85fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f77c88-cdfd-4003-9280-a8f739494544",
   "metadata": {},
   "source": [
    "# CPU vs GPU Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b78553-6997-47c4-a05b-8fb17adb78e7",
   "metadata": {},
   "source": [
    "When I first started training, I was using my CPU (Intel Core-i9 11900KF) and the projected training time for one model was about 1 day (using 50 epochs).  I knew this was not going to allow me many models or flexiblity in changes so I enabled my GPU. I ended up looking at a lot of tutorials but ended up following the instructions from Bex T. [7] I was able to get the drivers from NVIDIA and enable TensorFlow to access my GPU (NVIDIA 3080 TI).  This allowed me to train the same model in  2.5 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de133a7d-4f34-418f-a6f0-c3294389fed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da118c0-7a16-43dd-8045-927de6d5c9a6",
   "metadata": {},
   "source": [
    "# Batching For System Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908cf931-f4a1-44f3-8e1b-5da47b208c66",
   "metadata": {},
   "source": [
    "The second hardware issue that I ran into was self inflicted.  Before I augmented my dataset, I was trying to load all of the numpy arrays the images created into memory, I was not batching anything.  This would max my system RAM out (64GB) and prevent the rest of the notebook from progressing.  Agarwal [1] had a great example on avoiding this issue.  First, rather than import the images and convert them to arrays immediately, he created a Pandas dataframe to hold the ages and the image file locations. Once he was ready to process the arrays, he would convert the images. He also utialized TensorFlows dataset process and how to batch that information. After I call my Pipeline function, I also batched the datasets to help with system memory.\n",
    "\n",
    "Once I got this system in place, I felt comfortable expanding my image dataset from 35,000 to 350,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8352be80-4e6c-4165-a9d4-8b9bafe95cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(99)\n",
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caefaad2-4cd7-4245-9da8-12af304a56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = pd.read_csv('faces_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ab4444-fb56-4f15-a11c-696db48a03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(file_df, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28656681-71a5-41db-a746-040febf6838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = list(train['File'])\n",
    "train_labels = list(train['Group'])\n",
    "\n",
    "test_files = list(test['File'])\n",
    "test_labels = list(test['Group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10300053-fe2f-40d6-ba08-d55e2685ee4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tensor = tf.constant(train_files)\n",
    "y_train_tensor = tf.constant(train_labels)\n",
    "\n",
    "X_test_tensor = tf.constant(test_files)\n",
    "y_test_tensor = tf.constant(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1607a2d3-9917-462b-b837-e7b356a8c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_groups = 6\n",
    "\n",
    "def pipeline(file, label):\n",
    "    image = tf.io.read_file(file)\n",
    "    image_gray = tf.io.decode_jpeg(image, channels=1) \n",
    "    image_resized = tf.image.resize(image_gray, [200,200])\n",
    "    label = tf.one_hot(label, num_groups)\n",
    "    return image_resized, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a509b-de57-44a4-b285-08b50706e45b",
   "metadata": {},
   "source": [
    "Below is where I batched the images into groups of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4208baf4-c62d-462b-b64a-a8da4bb26ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cstod\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train_tensor))\n",
    "train_dataset = train_dataset.map(pipeline)\n",
    "train_dataset = train_dataset.batch(64)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_tensor, y_test_tensor))\n",
    "test_dataset = test_dataset.map(pipeline)\n",
    "test_dataset = test_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-helen",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "agarwal = Sequential()\n",
    "\n",
    "agarwal.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(200, 200, 1)))    # 3rd dim = 1 for grayscale images.\n",
    "agarwal.add(AveragePooling2D(pool_size=(2,2)))\n",
    "\n",
    "agarwal.add(Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "agarwal.add(AveragePooling2D(pool_size=(2,2)))\n",
    "\n",
    "agarwal.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "agarwal.add(AveragePooling2D(pool_size=(2,2)))\n",
    "\n",
    "agarwal.add(Conv2D(filters=256, kernel_size=3, activation='relu'))\n",
    "agarwal.add(AveragePooling2D(pool_size=(2,2)))\n",
    "\n",
    "agarwal.add(GlobalAveragePooling2D())\n",
    "\n",
    "agarwal.add(Dense(132, activation='relu'))\n",
    "\n",
    "agarwal.add(Dense(6, activation='softmax'))\n",
    "\n",
    "agarwal.summary()\n",
    "\n",
    "model_name='prerakAdam1e4'\n",
    "epoch_size = 20\n",
    "\n",
    "#Model from Agarwal [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3e97e7-440a-46c1-bb25-1577a36a31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_net = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(200, 200, 1)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "#Using the AlexNet CNN model [4].\n",
    "model_name='nonAugAlexNetSGD1e4'\n",
    "epoch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed777cd-ffcd-48b4-980e-c65ea8ffead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_log_dir = os.path.join(os.curdir, \"logs\\\\fit\\\\\")\n",
    "\n",
    "def get_log_dir():\n",
    "    file_id = time.strftime('run_%Y_%m_%d-%H_%M')\n",
    "    return os.path.join(base_log_dir, file_id)\n",
    "\n",
    "log_dir = get_log_dir()\n",
    "tensorboard  = tf.keras.callbacks.TensorBoard(log_dir)\n",
    "#Adapted from Alake [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "agarwal.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-bahamas",
   "metadata": {},
   "source": [
    "The hyperparameters that I used were mostly Adam, Adamax and SGD optimizers, learning rate between 1e-3 and 1e-5, landing on 1e-4 being the overall best metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f963b98e-060c-47f9-bf7b-94d0a4f9fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_net.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35966fca-d501-4854-8b17-70cc59cee376",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=r\"C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\",\n",
    "                             monitor='val_accuracy',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-teacher",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agarwal_history = agarwal.fit(train_dataset,\n",
    "                                  batch_size=512,\n",
    "                                  validation_data=test_dataset,\n",
    "                                  epochs=20,\n",
    "                                  callbacks=[tensorboard, checkpoint]\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "797717f0-198b-42b2-b6bc-d1d52ce7e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.0827 - accuracy: 0.1938\n",
      "Epoch 1: val_accuracy improved from -inf to 0.22541, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 22s 37ms/step - loss: 3.0827 - accuracy: 0.1938 - val_loss: 1.8412 - val_accuracy: 0.2254\n",
      "Epoch 2/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.7144 - accuracy: 0.2092\n",
      "Epoch 2: val_accuracy improved from 0.22541 to 0.23897, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 32ms/step - loss: 2.7144 - accuracy: 0.2092 - val_loss: 1.8557 - val_accuracy: 0.2390\n",
      "Epoch 3/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.4862 - accuracy: 0.2191\n",
      "Epoch 3: val_accuracy improved from 0.23897 to 0.24557, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 32ms/step - loss: 2.4862 - accuracy: 0.2191 - val_loss: 1.9440 - val_accuracy: 0.2456\n",
      "Epoch 4/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.3706 - accuracy: 0.2281\n",
      "Epoch 4: val_accuracy improved from 0.24557 to 0.28157, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 32ms/step - loss: 2.3706 - accuracy: 0.2281 - val_loss: 1.7290 - val_accuracy: 0.2816\n",
      "Epoch 5/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.2602 - accuracy: 0.2377\n",
      "Epoch 5: val_accuracy improved from 0.28157 to 0.28553, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 32ms/step - loss: 2.2602 - accuracy: 0.2377 - val_loss: 1.7330 - val_accuracy: 0.2855\n",
      "Epoch 6/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.1826 - accuracy: 0.2448\n",
      "Epoch 6: val_accuracy improved from 0.28553 to 0.30117, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 32ms/step - loss: 2.1826 - accuracy: 0.2448 - val_loss: 1.7116 - val_accuracy: 0.3012\n",
      "Epoch 7/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.1230 - accuracy: 0.2550\n",
      "Epoch 7: val_accuracy improved from 0.30117 to 0.30362, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 32ms/step - loss: 2.1230 - accuracy: 0.2550 - val_loss: 1.6864 - val_accuracy: 0.3036\n",
      "Epoch 8/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.0556 - accuracy: 0.2657\n",
      "Epoch 8: val_accuracy improved from 0.30362 to 0.31926, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 32ms/step - loss: 2.0556 - accuracy: 0.2657 - val_loss: 1.6685 - val_accuracy: 0.3193\n",
      "Epoch 9/20\n",
      "469/470 [============================>.] - ETA: 0s - loss: 2.0046 - accuracy: 0.2723\n",
      "Epoch 9: val_accuracy improved from 0.31926 to 0.32397, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 32ms/step - loss: 2.0044 - accuracy: 0.2724 - val_loss: 1.6519 - val_accuracy: 0.3240\n",
      "Epoch 10/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.9639 - accuracy: 0.2785\n",
      "Epoch 10: val_accuracy did not improve from 0.32397\n",
      "470/470 [==============================] - 14s 29ms/step - loss: 1.9639 - accuracy: 0.2785 - val_loss: 1.6638 - val_accuracy: 0.3162\n",
      "Epoch 11/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.9342 - accuracy: 0.2811\n",
      "Epoch 11: val_accuracy improved from 0.32397 to 0.33302, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 31ms/step - loss: 1.9342 - accuracy: 0.2811 - val_loss: 1.6346 - val_accuracy: 0.3330\n",
      "Epoch 12/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.8888 - accuracy: 0.2898\n",
      "Epoch 12: val_accuracy improved from 0.33302 to 0.33660, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 31ms/step - loss: 1.8888 - accuracy: 0.2898 - val_loss: 1.6265 - val_accuracy: 0.3366\n",
      "Epoch 13/20\n",
      "469/470 [============================>.] - ETA: 0s - loss: 1.8589 - accuracy: 0.2941\n",
      "Epoch 13: val_accuracy improved from 0.33660 to 0.33999, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 31ms/step - loss: 1.8588 - accuracy: 0.2943 - val_loss: 1.5983 - val_accuracy: 0.3400\n",
      "Epoch 14/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.8263 - accuracy: 0.3036\n",
      "Epoch 14: val_accuracy did not improve from 0.33999\n",
      "470/470 [==============================] - 14s 29ms/step - loss: 1.8263 - accuracy: 0.3036 - val_loss: 1.6343 - val_accuracy: 0.3292\n",
      "Epoch 15/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.8014 - accuracy: 0.3125\n",
      "Epoch 15: val_accuracy improved from 0.33999 to 0.34753, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 31ms/step - loss: 1.8014 - accuracy: 0.3125 - val_loss: 1.5803 - val_accuracy: 0.3475\n",
      "Epoch 16/20\n",
      "468/470 [============================>.] - ETA: 0s - loss: 1.7810 - accuracy: 0.3178\n",
      "Epoch 16: val_accuracy improved from 0.34753 to 0.35884, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 32ms/step - loss: 1.7799 - accuracy: 0.3181 - val_loss: 1.5824 - val_accuracy: 0.3588\n",
      "Epoch 17/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.7530 - accuracy: 0.3243\n",
      "Epoch 17: val_accuracy did not improve from 0.35884\n",
      "470/470 [==============================] - 13s 28ms/step - loss: 1.7530 - accuracy: 0.3243 - val_loss: 1.5708 - val_accuracy: 0.3534\n",
      "Epoch 18/20\n",
      "469/470 [============================>.] - ETA: 0s - loss: 1.7198 - accuracy: 0.3306\n",
      "Epoch 18: val_accuracy did not improve from 0.35884\n",
      "470/470 [==============================] - 14s 30ms/step - loss: 1.7196 - accuracy: 0.3308 - val_loss: 1.5607 - val_accuracy: 0.3581\n",
      "Epoch 19/20\n",
      "469/470 [============================>.] - ETA: 0s - loss: 1.7076 - accuracy: 0.3366\n",
      "Epoch 19: val_accuracy improved from 0.35884 to 0.36223, saving model to C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cstod\\GradFiles\\Captstone\\.ipynb_checkpoints\\assets\n",
      "470/470 [==============================] - 15s 33ms/step - loss: 1.7076 - accuracy: 0.3367 - val_loss: 1.5485 - val_accuracy: 0.3622\n",
      "Epoch 20/20\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.6834 - accuracy: 0.3416\n",
      "Epoch 20: val_accuracy did not improve from 0.36223\n",
      "470/470 [==============================] - 13s 28ms/step - loss: 1.6834 - accuracy: 0.3416 - val_loss: 1.5526 - val_accuracy: 0.3511\n"
     ]
    }
   ],
   "source": [
    "alex_net_history = alex_net.fit(train_dataset,\n",
    "                                batch_size=64,\n",
    "                                validation_data=test_dataset,\n",
    "                                epochs=epoch_size,\n",
    "                                callbacks=[tensorboard, checkpoint]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "agarwal.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9df6e0be-6cbc-410f-a53a-5d7d66ff36cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x13275f26640>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_net.load_weights('.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11a88098-3200-4afd-b70d-a5fb360e85e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 1s 12ms/step - loss: 1.5485 - accuracy: 0.3622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5485175848007202, 0.36223143339157104]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_net.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0513d394-c089-4093-b434-0c251fae4817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: nonAugAlexNetSGD1e4_20e\\assets\n"
     ]
    }
   ],
   "source": [
    "alex_net.save(f'{model_name}_{epoch_size}e')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
